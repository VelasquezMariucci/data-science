{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfGqhoqHuuIJWeKDK1pm4s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Prompt: I'm tasked with designing a college level study plan for learning LLMs. I'm a university student studying Computer Engineering but without prior experience with deep learning. I want to become proficient in understanding and applying LLMs within 4 months. The plan should be divided by school weeks with incremental goals, material and practical excercises.You are an experienced professor of Machine Learning who has taught both undergraduates and master's students, and you can adapt to each student's needs. Present a study plan as numbered list with bullet points under each week. Include references and practical exercises. The tone should be educational, supportive, and motivational."],"metadata":{"id":"bnY4FOgtIsa8"}},{"cell_type":"markdown","metadata":{"id":"4faab98f"},"source":["Here is a possible plan to help you learn about LLMs:\n","\n","**Month 1: Deep Learning Fundamentals**\n","\n","*   **Week 1: Introduction to Machine Learning and Neural Networks**\n","    *   **Goal:** Understand the basic concepts of machine learning and the architecture of simple neural networks.\n","    *   **Material:**\n","        *   Andrew Ng's Machine Learning course on Coursera (focus on the first few weeks).\n","        *   Chapter 1-3 of \"Deep Learning\" by Goodfellow, Bengio, and Courville.\n","    *   **Practical Exercise:** Implement a simple linear regression or logistic regression model using NumPy.\n","*   **Week 2: Backpropagation and Training Neural Networks**\n","    *   **Goal:** Grasp the backpropagation algorithm and how to train neural networks effectively.\n","    *   **Material:**\n","        *   Andrew Ng's Deep Learning Specialization on Coursera (focus on Week 1-2).\n","        *   Chapter 4-6 of \"Deep Learning\" by Goodfellow, Bengio, and Courville.\n","    *   **Practical Exercise:** Implement a simple feedforward neural network and train it on a dataset like MNIST using NumPy or a basic framework like PyTorch or TensorFlow.\n","*   **Week 3: Convolutional Neural Networks (CNNs)**\n","    *   **Goal:** Understand the architecture and applications of CNNs for image processing.\n","    *   **Material:**\n","        *   Andrew Ng's Deep Learning Specialization on Coursera (focus on Week 3).\n","        *   Chapter 9 of \"Deep Learning\" by Goodfellow, Bengio, and Courville.\n","    *   **Practical Exercise:** Build and train a CNN for image classification using PyTorch or TensorFlow.\n","*   **Week 4: Recurrent Neural Networks (RNNs) and Sequence Models**\n","    *   **Goal:** Learn about RNNs and their use in processing sequential data.\n","    *   **Material:**\n","        *   Andrew Ng's Deep Learning Specialization on Coursera (focus on Week 5).\n","        *   Chapter 10 of \"Deep Learning\" by Goodfellow, Bengio, and Courville.\n","    *   **Practical Exercise:** Implement a simple RNN for text generation or sentiment analysis.\n","\n","**Month 2: Introduction to Natural Language Processing (NLP) and LLMs**\n","\n","*   **Week 5: Fundamentals of NLP**\n","    *   **Goal:** Get familiar with basic NLP tasks and techniques like tokenization, stemming, and lemmatization.\n","    *   **Material:**\n","        *   Jurafsky and Martin's \"Speech and Language Processing\" (select chapters).\n","        *   NLTK or spaCy documentation and tutorials.\n","    *   **Practical Exercise:** Use NLTK or spaCy to perform basic text preprocessing on a dataset.\n","*   **Week 6: Word Embeddings**\n","    *   **Goal:** Understand word embeddings like Word2Vec, GloVe, and FastText.\n","    *   **Material:**\n","        *   Online tutorials and articles on word embeddings.\n","        *   Mikolov et al.'s original papers on Word2Vec.\n","    *   **Practical Exercise:** Train your own word embeddings on a small text corpus or use pre-trained embeddings.\n","*   **Week 7: Attention Mechanisms and Transformers**\n","    *   **Goal:** Dive deep into the attention mechanism and the Transformer architecture.\n","    *   **Material:**\n","        *   Vaswani et al.'s \"Attention Is All You Need\" paper.\n","        *   \"The Illustrated Transformer\" blog post by Jay Alammar.\n","    *   **Practical Exercise:** Implement the attention mechanism or a simplified Transformer block.\n","*   **Week 8: Introduction to Large Language Models (LLMs)**\n","    *   **Goal:** Understand what LLMs are, their history, and their capabilities.\n","    *   **Material:**\n","        *   Survey papers and articles on LLMs.\n","        *   Blog posts from OpenAI, Google AI, and other research labs.\n","    *   **Practical Exercise:** Experiment with a pre-trained LLM through an API (e.g., OpenAI API, Google AI Studio).\n","\n","**Month 3: LLM Architectures and Fine-tuning**\n","\n","*   **Week 9: Popular LLM Architectures**\n","    *   **Goal:** Explore different LLM architectures like GPT, BERT, and T5.\n","    *   **Material:**\n","        *   Original papers on GPT, BERT, and T5.\n","        *   Hugging Face Transformers library documentation.\n","    *   **Practical Exercise:** Load and experiment with pre-trained models from the Hugging Face Transformers library.\n","*   **Week 10: Fine-tuning LLMs**\n","    *   **Goal:** Learn how to fine-tune pre-trained LLMs for specific downstream tasks.\n","    *   **Material:**\n","        *   Hugging Face Transformers fine-tuning tutorials.\n","        *   Papers and articles on fine-tuning techniques.\n","    *   **Practical Exercise:** Fine-tune a pre-trained LLM on a text classification or sequence labeling task.\n","*   **Week 11: Parameter-Efficient Fine-Tuning (PEFT)**\n","    *   **Goal:** Understand techniques like LoRA and QLoRA for efficient fine-tuning.\n","    *   **Material:**\n","        *   Papers and tutorials on PEFT methods.\n","        *   Hugging Face PEFT library documentation.\n","    *   **Practical Exercise:** Apply PEFT techniques to fine-tune an LLM.\n","*   **Week 12: Prompt Engineering**\n","    *   **Goal:** Learn how to effectively design prompts to get desired outputs from LLMs.\n","    *   **Material:**\n","        *   Online guides and articles on prompt engineering.\n","        *   Experimentation with different prompting strategies.\n","    *   **Practical Exercise:** Practice prompt engineering with a pre-trained LLM for various tasks.\n","\n","**Month 4: Advanced LLM Concepts and Applications**\n","\n","*   **Week 13: Generative Models and Sampling Techniques**\n","    *   **Goal:** Explore different generative models and sampling strategies used in LLMs.\n","    *   **Material:**\n","        *   Papers and articles on generative models (e.g., variational autoencoders, generative adversarial networks - briefly).\n","        *   Hugging Face documentation on generation strategies.\n","    *   **Practical Exercise:** Experiment with different sampling techniques when generating text from an LLM.\n","*   **Week 14: Evaluating LLMs**\n","    *   **Goal:** Understand various metrics and methods for evaluating LLM performance.\n","    *   **Material:**\n","        *   Papers and articles on LLM evaluation benchmarks (e.g., GLUE, SuperGLUE, MMLU).\n","        *   Hugging Face Evaluate library.\n","    *   **Practical Exercise:** Evaluate a fine-tuned LLM on a benchmark dataset.\n","*   **Week 15: Ethical Considerations and Bias in LLMs**\n","    *   **Goal:** Become aware of the ethical implications and potential biases in LLMs.\n","    *   **Material:**\n","        *   Papers and articles on fairness, accountability, and transparency in AI.\n","        *   Discussions and case studies on LLM bias.\n","    *   **Practical Exercise:** Analyze the output of an LLM for potential biases.\n","*   **Week 16: Exploring Advanced Topics and Projects**\n","    *   **Goal:** Delve into more advanced topics like reinforcement learning from human feedback (RLHF), constitutional AI, or explore a personal LLM project.\n","    *   **Material:**\n","        *   Recent research papers and preprints.\n","        *   Online resources and tutorials on advanced topics.\n","    *   **Practical Exercise:** Start working on a small LLM-related project that interests you (e.g., building a chatbot, summarizing text, generating code).\n","\n","**References:**\n","\n","*   \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\n","*   \"Speech and Language Processing\" by Daniel Jurafsky and James H. Martin\n","*   Andrew Ng's Machine Learning and Deep Learning courses on Coursera\n","*   Hugging Face Transformers and PEFT libraries documentation\n","*   Online resources, research papers, and blog posts from leading AI labs.\n","\n","Remember to be consistent, practice regularly, and don't be afraid to explore topics that pique your interest. You've got this!"]},{"cell_type":"markdown","source":["Bayesian Analysis:\n","\n","Bayesian analysis is a statistical methos that uses Baye's theorem to update beliefes about unknown parameters when new data surges. It treats the parameters as random variables and sets them in a probability distribution to represent their uncertainty.\n","\n","1) Big idea:\n","\n","After seeing the new data, you update what you believed before by how compatible the data are with each possibility, and then normalize by the overall chance of seeing the data like this.\n","\n","2) Formula:\n","\n","P(0|data) = (P(data|0) * P(0)) / P(data)\n","\n","Where:\n","- P(0): prior belief about parameter\n","- P(data∣0): likelihood\n","- P(data)=∫P(data∣0) P(0) d0 evidence normalized\n","- P(0∣data): updated belief\n","\n","3) Formula in text:\n","\n","Posterior = Likelihood × Prior ÷ Evidence.\n","\n","4) Mathematical Example:\n","\n","- Prevalence = P(Disease) = 0.01\n","- Sentitivity = P(+|Disease) = 0.9\n","- Specificity = P(-|Disease) = 0.05\n","\n","Step 1:\n","\n","P(+) = P(+|Disease) * P(Disease) + P(+|Disease) * P(-Disease)\n","\n","P(+) = 0.9 * 0.01 + 0.05 * 0.99 = 0.0585\n","\n","Step 2:\n","\n","P(0|Disease) = (P(Disease|0) * P(0)) / P(Disease)\n","\n","P(Disease|+) = 0.9 * 0.01 / 0.0585 = 0.154\n"],"metadata":{"id":"FZI0Lll1LyvX"}},{"cell_type":"code","source":["# Parameters\n","prevalence = 0.01        # P(D)\n","sensitivity = 0.90       # P(+|D)\n","specificity = 0.95       # P(-|¬D)\n","false_positive_rate = 1 - specificity\n","\n","# Step 1: Evidence (probability of a positive test)\n","p_positive = sensitivity * prevalence + false_positive_rate * (1 - prevalence)\n","\n","# Step 2: Posterior (probability of disease given positive test)\n","posterior = (sensitivity * prevalence) / p_positive\n","\n","print(f\"Probability of disease given positive test: {posterior:.3f}\")"],"metadata":{"id":"_o14I4HgQ87R","executionInfo":{"status":"ok","timestamp":1757552044996,"user_tz":-480,"elapsed":18,"user":{"displayName":"Esteban Velasquez","userId":"02615534236360998258"}},"outputId":"4e126344-eec0-441d-a425-315437d13f57","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Probability of disease given positive test: 0.154\n"]}]}]}