{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxm2BasabIFe"
   },
   "source": [
    "#HW5\n",
    "##Esteban Velasquez\n",
    "##114502904\n",
    "##02/10/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 8674,
     "status": "error",
     "timestamp": 1759366730655,
     "user": {
      "displayName": "Esteban Velasquez",
      "userId": "02615534236360998258"
     },
     "user_tz": -480
    },
    "id": "eM1j7wofQ6z1",
    "outputId": "fc74c676-d5f1-4340-aa86-7dff7606bc88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model mistralai/Mistral-7B-Instruct-v0.2 is not supported for task text-generation and provider featherless-ai. Supported task: conversational.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-195071905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \"\"\"\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mtext_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2370\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2371\u001b[0m         \u001b[0mprovider_helper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2372\u001b[0;31m         request_parameters = provider_helper.prepare_request(\n\u001b[0m\u001b[1;32m   2373\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_providers/_common.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, inputs, parameters, headers, model, api_key, extra_payload)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# mapped model from HF model ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mprovider_mapping_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_mapping_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# default HF headers + user headers (to customize in subclasses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/inference/_providers/_common.py\u001b[0m in \u001b[0;36m_prepare_mapping_info\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprovider_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;34mf\"Model {model} is not supported for task {self.task} and provider {self.provider}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;34mf\"Supported task: {provider_mapping.task}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Model mistralai/Mistral-7B-Instruct-v0.2 is not supported for task text-generation and provider featherless-ai. Supported task: conversational."
     ]
    }
   ],
   "source": [
    "!pip install --quiet huggingface_hub pandas\n",
    "from huggingface_hub import InferenceClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Setup HF API\n",
    "# --------------------------\n",
    "HF_API_KEY = \"key\"  # ðŸ‘ˆ paste your key here\n",
    "client = InferenceClient(model=\"mistralai/Mistral-7B-Instruct-v0.2\", token=HF_API_KEY)\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Load Dataset\n",
    "# --------------------------\n",
    "drive.mount('/content/drive')\n",
    "file_path = \"/content/drive/My Drive/googleplaystore.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Clean Data\n",
    "# --------------------------\n",
    "df['Installs'] = df['Installs'].astype(str).str.replace('[+,]', '', regex=True)\n",
    "df['Installs'] = pd.to_numeric(df['Installs'], errors='coerce')\n",
    "df['Reviews'] = pd.to_numeric(df['Reviews'], errors='coerce')\n",
    "\n",
    "if 'Price' in df.columns:\n",
    "    df['Price'] = df['Price'].astype(str).str.replace('$', '', regex=False)\n",
    "    df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "\n",
    "def size_to_num(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x.endswith('M'):\n",
    "            return float(x[:-1]) * 1e6\n",
    "        elif x.endswith('k'):\n",
    "            return float(x[:-1]) * 1e3\n",
    "        elif x == 'Varies with device':\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "if 'Size' in df.columns:\n",
    "    df['Size'] = df['Size'].apply(size_to_num)\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Summarize Dataset\n",
    "# --------------------------\n",
    "num_vars = df.select_dtypes(include=['int64','float64']).columns\n",
    "cat_vars = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "Google Play Store dataset:\n",
    "- Shape: {df.shape}\n",
    "- Numerical variables: {list(num_vars)}\n",
    "- Categorical variables: {list(cat_vars)}\n",
    "- Numerical summary:\n",
    "{df[num_vars].describe().to_string()}\n",
    "- Missing values:\n",
    "{df[num_vars].isnull().sum().to_dict()}\n",
    "- Categorical counts (top 3 per column):\n",
    "{ {col: df[col].value_counts().head(3).to_dict() for col in cat_vars} }\n",
    "- Top Genres per Category (sample):\n",
    "{df.groupby(\"Category\")[\"Genres\"].value_counts().head(10).to_dict()}\n",
    "- Correlation Installs vs Reviews:\n",
    "{df['Installs'].corr(df['Reviews'])}\n",
    "- Top 3 Categories:\n",
    "{df['Category'].value_counts().head(3).to_dict()}\n",
    "- Top 3 Highest Rated Apps:\n",
    "{df.sort_values(by='Rating', ascending=False).head(3)[['App','Rating','Installs']].to_dict(orient='records')}\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------\n",
    "# Step 5: Send to HF Chat LLM\n",
    "# --------------------------\n",
    "prompt = f\"\"\"\n",
    "Here is a dataset summary:\n",
    "\n",
    "{summary_text}\n",
    "\n",
    "Please answer in clear English:\n",
    "1. Overview of the dataset\n",
    "2. How many numerical vs categorical variables\n",
    "3. Summary statistics for numerical variables (mean, std, min, max, missing)\n",
    "4. Category counts for categorical variables\n",
    "5. Relationship between Genres and Category\n",
    "6. Relationship between Installs and Reviews\n",
    "7. Top 3 most popular Categories (popularity = # of apps)\n",
    "8. Top 3 apps with highest ratings â€” does install count matter?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=800,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message[\"content\"])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNb14hbNoaBsrwfRs+qjJdZ",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
